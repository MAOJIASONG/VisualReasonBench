#!/usr/bin/env python3
"""
Puzzle Translater æµ‹è¯•ä»»åŠ¡æ¼”ç¤º
è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ‹¼å›¾ä»»åŠ¡ï¼ŒåŒ…å«ä¸¤ä¸ªæ­¥éª¤ï¼š
1. å°† Object #7 (ID: 2) ç§»åŠ¨åˆ°é»‘è‰²æ¡†ï¼ˆå®¹å™¨ï¼‰ä¸­
2. å°† Object #6 (ID: 3) æ”¾åœ¨ ID 2 çš„ä¸Šé¢
"""
import os
from pathlib import Path

from phyvpuzzle import load_config, BenchmarkRunner, validate_config

# å°è¯•ä».envæ–‡ä»¶åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    print("æ³¨æ„ï¼špython-dotenv ä¸å¯ç”¨ã€‚ä»…ä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡ã€‚")

def check_api_keys():
    """æ£€æŸ¥APIå¯†é’¥æ˜¯å¦è®¾ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ‰“å°è­¦å‘Šã€‚"""
    if not os.getenv("OPENAI_API_KEY"):
        print("\n" + "="*80)
        print("âš ï¸  è­¦å‘Šï¼šç¯å¢ƒå˜é‡ä¸­æœªæ‰¾åˆ° OPENAI_API_KEYã€‚")
        print("è¯·åœ¨æ ¹ç›®å½•ä¸‹åˆ›å»º .env æ–‡ä»¶å¹¶æ·»åŠ ä½ çš„ API å¯†é’¥ï¼š")
        print("  OPENAI_API_KEY='your-key-here'")
        print("æˆ–è€…å°†å…¶å¯¼å‡ºä¸ºç¯å¢ƒå˜é‡ã€‚")
        print("æ²¡æœ‰å®ƒï¼Œè„šæœ¬å¯èƒ½ä¼šå¤±è´¥ã€‚")
        print("="*80 + "\n")

def main():
    """è¿è¡Œ Puzzle Translater æµ‹è¯•ä»»åŠ¡çš„ä¸»å‡½æ•°ã€‚"""
    check_api_keys()

    print("\n" + " ğŸ§© Puzzle Translater æµ‹è¯•ä»»åŠ¡ ".center(80, "="))
    
    # ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
    config_path = Path(__file__).resolve().parent.parent / "eval_configs" / "puzzle_translater.yaml"
    
    if not config_path.exists():
        print(f"âŒ æœªæ‰¾åˆ°é…ç½®æ–‡ä»¶ï¼š{config_path}")
        return 1
        
    try:
        config = load_config(str(config_path))
        validation_errors = validate_config(config)
        if validation_errors:
            print(f"âŒ é…ç½®æ— æ•ˆï¼š{validation_errors}")
            raise ValueError("é…ç½®æ— æ•ˆ")
        
        print("\n" + "ğŸ“‹ å®éªŒé…ç½®".center(50, "-"))
        print(f"  å®éªŒåç§°        : {config.runner.experiment_name}")
        print(f"  æ™ºèƒ½ä½“ï¼ˆæ¨¡å‹ï¼‰  : {config.agent.model_name}")
        print(f"  ä»»åŠ¡ç±»å‹        : {config.task.type} ({config.task.difficulty.value})")
        print(f"  æœ€å¤§æ­¥æ•°        : {config.environment.max_steps}")
        print("-" * 50)
        
        # åˆå§‹åŒ–å¹¶è®¾ç½®åŸºå‡†æµ‹è¯•è¿è¡Œå™¨
        print("\nğŸš€ åˆå§‹åŒ–æ‹¼å›¾åŸºå‡†æµ‹è¯•è¿è¡Œå™¨...")
        runner = BenchmarkRunner(config)
        
        print("\nğŸ¯ ä»»åŠ¡æ¦‚è§ˆï¼š")
        print("  â€¢ æ­¥éª¤ 1ï¼šå°† Object #7 (ID: 2) ç§»åŠ¨åˆ°é»‘è‰²æ¡†ï¼ˆå®¹å™¨ï¼‰ä¸­")
        print("  â€¢ æ­¥éª¤ 2ï¼šå°† Object #6 (ID: 3) æ”¾åœ¨ ID 2 çš„ä¸Šé¢")
        print("  â€¢ è¿™æ˜¯ä¸€ä¸ªç®€å•çš„å †å ä»»åŠ¡ï¼Œç”¨äºæµ‹è¯•åŸºæœ¬æ“ä½œèƒ½åŠ›")
        print("\n" + "ğŸ® å¼€å§‹ Puzzle Translater æŒ‘æˆ˜...".center(60, "-"))
        
        # --- è¿è¡ŒåŸºå‡†æµ‹è¯• ---
        try:
            evaluation_result = runner.run_benchmark(num_runs=1)
        except Exception as benchmark_error:
            print(f"\nâŒ åŸºå‡†æµ‹è¯•æ‰§è¡Œå¤±è´¥ï¼š{benchmark_error}")
            import traceback
            traceback.print_exc()
            return 1

        # --- æœ€ç»ˆæ€»ç»“ ---
        print("\n" + "ğŸ Puzzle Translater æŒ‘æˆ˜ç»“æœ".center(80, "="))
        
        if evaluation_result.accuracy > 0.5:
            print("ğŸ‰ æˆåŠŸï¼ä»»åŠ¡å·²æˆåŠŸå®Œæˆï¼")
        else:
            print("ğŸ˜” æŒ‘æˆ˜æœªå®Œæˆã€‚ä¸‹æ¬¡å¥½è¿ï¼")
            
        print(f"\nğŸ“Š æ€§èƒ½æŒ‡æ ‡ï¼š")
        print(f"  â€¢ æˆåŠŸç‡: {evaluation_result.accuracy:.1%}")
        if evaluation_result.pass_at_k:
            for k, rate in evaluation_result.pass_at_k.items():
                print(f"  â€¢ Pass@{k}: {rate:.1%}")
        if evaluation_result.token_efficiency != float('inf'):
            print(f"  â€¢ Token æ•ˆç‡: {evaluation_result.token_efficiency:.0f} tokens/æˆåŠŸ")
        if evaluation_result.distance_to_optimal != float('inf'):
            print(f"  â€¢ æ­¥éª¤æ•ˆç‡: {evaluation_result.distance_to_optimal:.2f}x æœ€ä¼˜")
            
        print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ï¼š")
        print(f"  â¡ï¸  æ—¥å¿—ç›®å½•    : {runner.logger.run_dir}")
        print(f"  â¡ï¸  ç»“æœExcel   : {runner.logger.run_dir}/{config.runner.results_excel_path}")
        print(f"  â¡ï¸  å®Œæ•´æ—¥å¿—    : {runner.logger.run_dir}/experiment_log.json")
        print(f"  â¡ï¸  å›¾åƒ        : {runner.logger.run_dir}/images/")
        print("=" * 80)
        
        return 0 if evaluation_result.accuracy > 0.5 else 1
        
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    exit(main())

