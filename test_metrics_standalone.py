#!/usr/bin/env python3
"""
Standalone Metrics Test - No Dependencies

Tests all implemented metrics without requiring PyBullet or other dependencies.
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from enum import Enum

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))


@dataclass
class TaskResult:
    """Result of a task execution."""
    success: bool
    final_score: float
    steps_taken: int
    time_taken: float
    error_message: Optional[str] = None
    metrics: Optional[Dict[str, float]] = None
    tokens_used: Optional[int] = None
    metadata: Optional[Dict[str, Any]] = None
    action_sequence: Optional[List[str]] = None
    task_id: Optional[str] = None


class MetricCalculator:
    """Calculate all metrics without dependencies."""
    
    @staticmethod
    def calculate_accuracy(results: List[TaskResult]) -> float:
        """Calculate accuracy metric."""
        if not results:
            return 0.0
        successful = sum(1 for r in results if r.success)
        return successful / len(results)
    
    @staticmethod
    def calculate_pass_at_k(results: List[TaskResult], k: int = 8) -> Tuple[float, int]:
        """Calculate Pass@K metric."""
        if not results:
            return 0.0, 0
        
        # Group results by task_id
        task_groups = {}
        for result in results:
            task_id = result.task_id or "default"
            if task_id not in task_groups:
                task_groups[task_id] = []
            task_groups[task_id].append(result)
        
        # Calculate pass@k
        successful_tasks = 0
        for task_id, task_results in task_groups.items():
            # Check if any of the first k attempts succeeded
            k_attempts = task_results[:k]
            if any(r.success for r in k_attempts):
                successful_tasks += 1
        
        pass_rate = successful_tasks / len(task_groups) if task_groups else 0.0
        return pass_rate, successful_tasks
    
    @staticmethod
    def calculate_avg_steps(results: List[TaskResult]) -> float:
        """Calculate average steps metric."""
        if not results:
            return 0.0
        total_steps = sum(r.steps_taken for r in results)
        return total_steps / len(results)
    
    @staticmethod
    def calculate_distance_to_optimal(results: List[TaskResult], 
                                     optimal_steps: List[int]) -> Tuple[float, float]:
        """
        Calculate distance to optimal steps.
        Returns: (average_distance, average_ratio)
        """
        if not results or not optimal_steps:
            return float('inf'), float('inf')
        
        # Extend optimal_steps if needed
        if len(results) > len(optimal_steps):
            num_runs = len(results) // len(optimal_steps)
            optimal_steps = optimal_steps * num_runs
        
        total_distance = 0.0
        total_ratio = 0.0
        count = 0
        
        for result, optimal in zip(results, optimal_steps):
            distance = abs(result.steps_taken - optimal)
            ratio = (result.steps_taken - optimal) / optimal if optimal > 0 else 0
            
            total_distance += distance
            total_ratio += ratio
            count += 1
        
        avg_distance = total_distance / count if count > 0 else float('inf')
        avg_ratio = total_ratio / count if count > 0 else float('inf')
        
        return avg_distance, avg_ratio
    
    @staticmethod
    def calculate_token_efficiency(results: List[TaskResult]) -> float:
        """Calculate token efficiency for successful tasks."""
        successful = [r for r in results if r.success]
        if not successful:
            return float('inf')
        
        total_tokens = 0
        for result in successful:
            if result.tokens_used:
                total_tokens += result.tokens_used
            else:
                # Estimate if not available
                total_tokens += result.steps_taken * 500
        
        return total_tokens / len(successful)


def test_openrouter_integration():
    """Test OpenRouter integration."""
    print("\n" + "="*60)
    print("OPENROUTER INTEGRATION TEST")
    print("="*60)
    
    try:
        # Import OpenRouter client
        from src.phyvpuzzle.models.openrouter_client import create_openrouter_client
        
        print("\n1. Testing automatic API key loading...")
        
        # Check for API key in various sources
        from dotenv import load_dotenv
        load_dotenv()
        
        api_key = (
            os.getenv("OPENROUTER_API_KEY") or
            os.getenv("API_KEY") or
            os.getenv("api_key")
        )
        
        if api_key:
            print("âœ… API key found in environment")
            
            # Create client
            print("\n2. Creating OpenRouter client...")
            client = create_openrouter_client()
            print("âœ… Client created successfully")
            
            # Test connection
            print("\n3. Testing API connection...")
            success, message = client.test_connection()
            if success:
                print(f"âœ… {message}")
                return True
            else:
                print(f"âš ï¸ {message}")
                return False
        else:
            print("âš ï¸ No API key found")
            print("\nTo enable OpenRouter:")
            print("1. Create a .env file in project root")
            print("2. Add: OPENROUTER_API_KEY=your-key-here")
            return False
            
    except ImportError as e:
        print(f"âš ï¸ Could not import OpenRouter client: {e}")
        return False
    except Exception as e:
        print(f"âš ï¸ Error testing OpenRouter: {e}")
        return False


def simulate_task_results(num_tasks: int = 5, 
                         runs_per_task: int = 8,
                         optimal_steps_list: List[int] = None) -> List[TaskResult]:
    """Simulate task execution results."""
    import random
    
    if optimal_steps_list is None:
        optimal_steps_list = [6, 8, 10, 7, 9]
    
    results = []
    
    for task_idx in range(num_tasks):
        task_id = f"puzzle_task_{task_idx}"
        optimal = optimal_steps_list[task_idx % len(optimal_steps_list)]
        
        for run_idx in range(runs_per_task):
            # Vary success rate by run
            success_rate = 0.5 + (run_idx * 0.05)
            success = random.random() < success_rate
            
            # Calculate steps
            if success:
                steps = optimal + random.randint(-1, 2)
            else:
                steps = optimal + random.randint(3, 8)
            
            # Calculate tokens (500-1500 per step)
            tokens = steps * random.randint(500, 1500)
            
            result = TaskResult(
                success=success,
                final_score=1.0 if success else random.uniform(0.3, 0.7),
                steps_taken=steps,
                time_taken=steps * random.uniform(1.5, 3.0),
                tokens_used=tokens,
                task_id=task_id,
                action_sequence=[f"action_{i}" for i in range(steps)]
            )
            results.append(result)
    
    return results


def main():
    """Main test function."""
    print("\nðŸš€ VisualReasonBench - Complete Metrics Implementation Test")
    print("="*60)
    
    # Test OpenRouter integration
    openrouter_available = test_openrouter_integration()
    
    # Test parameters
    num_tasks = 5
    runs_per_task = 8
    optimal_steps = [6, 8, 10, 7, 9]
    
    print("\n" + "="*60)
    print("METRICS EVALUATION")
    print("="*60)
    
    print(f"\nðŸ“Š Configuration:")
    print(f"  - Tasks: {num_tasks}")
    print(f"  - Runs per task: {runs_per_task} (for Pass@8)")
    print(f"  - Optimal steps: {optimal_steps}")
    
    # Generate results
    print("\nðŸŽ¯ Generating simulated results...")
    results = simulate_task_results(num_tasks, runs_per_task, optimal_steps)
    print(f"âœ… Generated {len(results)} results")
    
    # Calculate metrics
    calculator = MetricCalculator()
    
    print("\nðŸ“ˆ METRICS RESULTS:")
    print("-" * 40)
    
    # 1. Accuracy
    accuracy = calculator.calculate_accuracy(results)
    successful = sum(1 for r in results if r.success)
    print(f"1. Accuracy (Acc): {accuracy:.3f}")
    print(f"   - Successful: {successful}/{len(results)}")
    
    # 2. Pass@8
    pass_at_8, passed_tasks = calculator.calculate_pass_at_k(results, k=8)
    print(f"\n2. Pass@8: {pass_at_8:.3f}")
    print(f"   - Tasks passed: {passed_tasks}/{num_tasks}")
    
    # 3. Average Steps
    avg_steps = calculator.calculate_avg_steps(results)
    print(f"\n3. Average Steps: {avg_steps:.2f}")
    
    # 4. Distance to Optimal
    distance, ratio = calculator.calculate_distance_to_optimal(results, optimal_steps)
    print(f"\n4. Distance to Optimal Steps:")
    print(f"   - Average distance: {distance:.2f} steps")
    print(f"   - Average ratio: {ratio:.3f}")
    print(f"   - Interpretation: {ratio*100:.1f}% deviation from optimal")
    
    # 5. Token Efficiency
    token_efficiency = calculator.calculate_token_efficiency(results)
    if token_efficiency != float('inf'):
        print(f"\n5. Token Efficiency: {token_efficiency:.0f} tokens/successful task")
        total_tokens = sum(r.tokens_used for r in results if r.tokens_used)
        print(f"   - Total tokens used: {total_tokens:,}")
    else:
        print(f"\n5. Token Efficiency: N/A (no successful tasks)")
    
    # Summary
    print("\n" + "="*60)
    print("IMPLEMENTATION SUMMARY")
    print("="*60)
    
    metrics_summary = {
        "accuracy": accuracy,
        "pass_at_8": pass_at_8,
        "avg_steps": avg_steps,
        "distance_to_optimal": distance,
        "distance_ratio": ratio,
        "token_efficiency": token_efficiency if token_efficiency != float('inf') else None
    }
    
    # Save results
    output_file = "metrics_test_results.json"
    with open(output_file, 'w') as f:
        json.dump({
            "configuration": {
                "num_tasks": num_tasks,
                "runs_per_task": runs_per_task,
                "optimal_steps": optimal_steps
            },
            "metrics": metrics_summary,
            "openrouter_available": openrouter_available,
            "timestamp": time.time()
        }, f, indent=2)
    
    print(f"\nðŸ“ Results saved to: {output_file}")
    
    print("\nâœ… IMPLEMENTATION STATUS:")
    print("  âœ… Acc (Accuracy) - Implemented")
    print("  âœ… Pass@8 - Implemented")
    print("  âœ… Avg Step - Implemented")
    print("  âœ… Distance to Optimal - Implemented (absolute & ratio)")
    print("  âœ… Token Efficiency - Implemented")
    print(f"  {'âœ…' if openrouter_available else 'âš ï¸'} OpenRouter Integration - {'Connected' if openrouter_available else 'Configured (API key needed)'}")
    
    print("\nðŸ“ FORMULA EXPLANATIONS:")
    print("  â€¢ Distance Ratio = (Actual - Optimal) / Optimal")
    print("  â€¢ Token Efficiency = Avg tokens for successful tasks")
    print("  â€¢ Pass@8 = % of tasks solved within 8 attempts")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())