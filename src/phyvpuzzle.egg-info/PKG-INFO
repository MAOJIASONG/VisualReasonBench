Metadata-Version: 2.4
Name: phyvpuzzle
Version: 0.1.0
Summary: A physical visual benchmark for evaluating VLM's pure visual-based sequential reasoning abilities
Author-email: Maojia Song <maojia_song@mymail.sutd.edu.sg>
License: MIT
Keywords: vision,reasoning,benchmark,VLM,3D,physics
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: pybullet>=3.2.0
Requires-Dist: opencv-python>=4.12.0
Requires-Dist: Pillow>=11.0.0
Requires-Dist: matplotlib>=3.10.0
Requires-Dist: pandas>=2.3.0
Requires-Dist: transformers>=4.54.0
Requires-Dist: openai<1.100,>=1.90.0
Requires-Dist: python-dotenv>=1.0.1
Requires-Dist: gymnasium>=1.2.0
Requires-Dist: scipy>=1.15.0
Requires-Dist: vllm==0.10.0
Requires-Dist: google-generativeai>=0.8.0
Requires-Dist: anthropic>=0.57.0
Requires-Dist: nltk>=3.9.0
Requires-Dist: nvitop>=1.5.0
Requires-Dist: aisuite>=0.1.0
Requires-Dist: tiktoken>=0.11.0
Requires-Dist: pip-tools>=7.5.0
Requires-Dist: litellm==1.75.3
Requires-Dist: notebook>=7.0.0
Requires-Dist: tenacity>=9.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: flake8>=5.0.0; extra == "dev"
Requires-Dist: mypy>=0.991; extra == "dev"
Requires-Dist: pre-commit>=2.20.0; extra == "dev"
Provides-Extra: training
Requires-Dist: accelerate>=1.8.0; extra == "training"
Requires-Dist: deepspeed>=0.17.0; extra == "training"
Requires-Dist: flash-attn>=2.8.3; extra == "training"
Requires-Dist: flashinfer-python>=0.2.0; extra == "training"
Requires-Dist: trl[vllm]==0.21.0; extra == "training"
Dynamic: license-file

# PhyVPuzzle: Physics-based Visual Reasoning Benchmark

> A comprehensive benchmark for evaluating physical visual reasoning capabilities of Vision-Language Models (VLMs) through interactive 3D physics simulation.

## ğŸ“‹ Table of Contents

- [Quick Start](#-quick-start)
- [Task Types](#-supported-task-types)
- [CLI Usage](#-cli-usage-guide)
- [Evaluation](#-detailed-evaluation-metrics)
- [Development](#-development-guide)

PhyVPuzzle evaluates Vision-Language Models on physics-based reasoning tasks through 3D simulation, testing perception, reasoning, planning, execution, and visual judgment capabilities.

## ğŸš€ Quick Start

```bash
# Install
git clone https://github.com/your-org/VisualReasonBench.git
cd VisualReasonBench
conda install -c conda-forge pybullet
pip install -e ".[dev]"

# Run example
python examples/domino_quick.py

# Run benchmark
phyvpuzzle benchmark --config eval_configs/domino_quick.yaml --num-runs 5
```

## ğŸ“Š Supported Task Types

| Task | Goal | Difficulty | Evaluation |
|------|------|------------|------------|
| ğŸ² **Domino** | Trigger chain reaction | 3-21 pieces | >80% fallen |
| ğŸ”’ **Luban Lock** | Assemble/disassemble puzzles | 3-15 pieces | Correct interlocking |
| ğŸ—ï¸ **Pagoda** | Build stable tower | Variable height | Stability & symmetry |

## âš™ï¸ Configuration

Key configuration parameters in `eval_configs/domino_quick.yaml`:
- **Agent**: Model (gpt-4o), temperature (0.7), max_tokens (500)
- **Environment**: PyBullet simulation with multi-view rendering
- **Task**: Type (domino), difficulty (very_easy), parameters (3 dominoes, line pattern)

## ğŸ› ï¸ CLI Usage Guide

```bash
# Basic usage
phyvpuzzle run --config config.yaml
phyvpuzzle evaluate --results-dir logs/
phyvpuzzle benchmark --config config.yaml --num-runs 5

# With GUI and parameter overrides
phyvpuzzle run --config config.yaml --gui --model gpt-4 --difficulty medium
```

## ğŸ“ Structure

```
src/phyvpuzzle/     # Core framework: agents, environments, tasks, evaluation
eval_configs/       # YAML configuration files  
examples/           # Usage examples
scripts/            # Benchmark and evaluation scripts
logs/               # Results and experiment data
```

## ğŸ”§ Development Guide

Extend the framework by inheriting from base classes:
- **Tasks**: `PuzzleTask` â†’ define prompts and validation
- **Environments**: `PhysicsEnvironment` â†’ setup 3D scenes  
- **Agents**: `VLMAgent` â†’ implement model interfaces
- **Metrics**: `MetricsCalculator` â†’ add evaluation functions

## ğŸ“Š Evaluation Metrics

**Core Metrics:**
- **Accuracy**: Success rate
- **Pass@K**: Success probability in K attempts  
- **Efficiency**: Steps/tokens per success
- **Optimality**: Distance from optimal solution

**Outputs:**
- Excel reports with detailed results (`logs/experiment_results.xlsx`)
- JSON trajectory data (`logs/detailed_reports/`)
- Multi-view screenshots (`logs/{experiment_name}/images/`)

## ğŸ“„ License & Contact

MIT License - **Maojia Song** (SUTD) - maojia_song@mymail.sutd.edu.sg

---

*PhyVPuzzle: Let AI think and act in the physical world* ğŸ¤–ğŸ§©
